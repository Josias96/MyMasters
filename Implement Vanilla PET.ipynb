{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Of The Art UNET\n",
    "## JA Engelbrecht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "from jupyterthemes import jtplot\n",
    "from skimage.util import montage as montage2d\n",
    "import UNets.Vanilla.UNet_Vanilla as UNet\n",
    "\n",
    "from MyFunctions.learningRateFunction import LRFinder\n",
    "from MyFunctions.CreatePaths import CreatePaths\n",
    "from MyFunctions.LoadImages import LoadImages\n",
    "from MyFunctions.RunModels import RunModels\n",
    "\n",
    "from CLR.clr_callback import *\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "############ Plot Images/Graphs Functions ############\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('mycmap', ['black', 'orange', 'red'])\n",
    "\n",
    "\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "mpl.rcParams.update({'font.size': 12})\n",
    "\n",
    "\n",
    "def set_size(width='thesis', fraction=1, subplots=(1, 1)):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float or string\n",
    "            Document width in points, or string of predined document type\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "    subplots: array-like, optional\n",
    "            The number of rows and columns of subplots.\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    if width == 'thesis':\n",
    "        width_pt = 398\n",
    "    else:\n",
    "        width_pt = width\n",
    "\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width_pt * fraction\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5**.5 - 1) / 2\n",
    "\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n",
    "\n",
    "    return (fig_width_in, fig_height_in)\n",
    "\n",
    "\n",
    "def showCTImage(IMG, SIZE):\n",
    "    plt.figure(figsize=(SIZE, SIZE))\n",
    "    plt.imshow(IMG, alpha=1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def showCTMontage(IMG, SIZE, SaveFig=False, save_fig_name=\"\"):\n",
    "    plt.figure(figsize=(SIZE, SIZE))\n",
    "    plt.imshow(montage2d(IMG), alpha=1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    if SaveFig:\n",
    "        save_fig_path = os.path.join(os.curdir, \"SavedFigures\")\n",
    "        plt.savefig(os.path.join(save_fig_path,\n",
    "                                 save_fig_name+\".pdf\"), bbox_inches='tight')\n",
    "\n",
    "\n",
    "def showCTMontageOverlay(IMG1, IMG2, SIZE=15, SaveFig=False, save_fig_name=\"\"):\n",
    "    fig, ax = plt.subplots(figsize=(SIZE, SIZE))\n",
    "    try:\n",
    "        ax.imshow(montage2d(IMG1), alpha=1, cmap='gray')\n",
    "    except:\n",
    "        print(\"Error: Img 1\")\n",
    "    try:\n",
    "        ax.imshow(montage2d(IMG2, fill=0), alpha=0.5,\n",
    "                  cmap=cmap, interpolation='none')\n",
    "    except:\n",
    "        print(\"Error: Img 2\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if SaveFig:\n",
    "        save_fig_path = os.path.join(os.curdir, \"SavedFigures\")\n",
    "        plt.savefig(os.path.join(save_fig_path,\n",
    "                                 save_fig_name+\".pdf\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: \tF://MyMasters//Data//TrainingData//PET//Heart//imgs\n",
      "Mask Path: \tF://MyMasters//Data//TrainingData//PET//Heart//masks\n",
      "Output Path: \tF://MyMasters//Output\n"
     ]
    }
   ],
   "source": [
    "# Change Class Variable below for different paths e.g. CT/PET\n",
    "path = CreatePaths(DeviceFlag=\"PC\", ScanTypeFlag=\"CT\", TrainTestFlag=\"Train\")\n",
    "\n",
    "#DATA_PATH = \"D://Masters_Repo//TrainingData//CT_v1\"\n",
    "#IMGS_PATH = path.imgPath()\n",
    "#MSKS_PATH = path.mskPath()\n",
    "#OUTPUT_PATH = path.outputPath()\n",
    "\n",
    "DATA_PATH = \"F://MyMasters//Data//TrainingData//PET\"\n",
    "IMGS_PATH = \"F://MyMasters//Data//TrainingData//PET//Heart//imgs\"\n",
    "MSKS_PATH = \"F://MyMasters//Data//TrainingData//PET//Heart//masks\"\n",
    "OUTPUT_PATH = \"F://MyMasters//Output\"\n",
    "\n",
    "ORIENTATION_ENSEMBLE = [\"Axial\", \"Sagittal\", \"Coronal\"]\n",
    "\n",
    "print(\"Image Path: \"+\"\\t\"+IMGS_PATH+\"\\n\"+\"Mask Path: \" +\n",
    "      \"\\t\"+MSKS_PATH+\"\\n\"+\"Output Path: \"+\"\\t\"+OUTPUT_PATH)\n",
    "\n",
    "ScanType = \"PET\"\n",
    "n_Scans = 25\n",
    "Orientation = \"Coronal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the following PET Images:\n",
      "CB_001_PET_M0.nii.gz\n",
      "CB_002_PET_M0.nii.gz\n",
      "CB_003_PET_M0.nii.gz\n",
      "CB_004_PET_M1.nii.gz\n",
      "CB_005_PET_M1.nii.gz\n",
      "CB_007_PET_M0.nii.gz\n",
      "CB_009_PET_M0.nii.gz\n",
      "CB_013_PET_M0.nii.gz\n",
      "CB_020_PET_M0.nii.gz\n",
      "CB_022_PET_M0.nii.gz\n",
      "CB_024_PET_M1.nii.gz\n",
      "CB_029_PET_M1.nii.gz\n",
      "CB_034_PET_M0.nii.gz\n",
      "CB_077_PET_M1.nii.gz\n",
      "CB_080_PET_M1.nii.gz\n",
      "CB_086_PET_M0.nii.gz\n",
      "CB_087_PET_M0.nii.gz\n",
      "CB_088_PET_M1.nii.gz\n",
      "CB_089_PET_M1.nii.gz\n",
      "CB_090_PET_M1.nii.gz\n",
      "CB_100_PET_M1.nii.gz\n",
      "CB_101_PET_M0.nii.gz\n",
      "CB_102_PET_M1.nii.gz\n",
      "CB_105_PET_M1.nii.gz\n",
      "CB_108_PET_M1.nii.gz\n",
      "Reading the following PET Masks:\n",
      "CB_001_PET_Heart_M0.nii.gz\n",
      "CB_002_PET_Heart_M0.nii.gz\n",
      "CB_003_PET_Heart_M0.nii.gz\n",
      "CB_004_PET_Heart_M1.nii.gz\n",
      "CB_005_PET_Heart_M1.nii.gz\n",
      "CB_007_PET_Heart_M0.nii.gz\n",
      "CB_009_PET_Heart_M0.nii.gz\n",
      "CB_013_PET_Heart_M0.nii.gz\n",
      "CB_020_PET_Heart_M0.nii.gz\n",
      "CB_022_PET_Heart_M0.nii.gz\n",
      "CB_024_PET_Heart_M1.nii.gz\n",
      "CB_029_PET_Heart_M1.nii.gz\n",
      "CB_034_PET_Heart_M0.nii.gz\n",
      "CB_077_PET_Heart_M1.nii.gz\n",
      "CB_080_PET_Heart_M1.nii.gz\n",
      "CB_086_PET_Heart_M0.nii.gz\n",
      "CB_087_PET_Heart_M0.nii.gz\n",
      "CB_088_PET_Heart_M1.nii.gz\n",
      "CB_089_PET_Heart_M1.nii.gz\n",
      "CB_090_PET_Heart_M1.nii.gz\n",
      "CB_100_PET_Heart_M1.nii.gz\n",
      "CB_101_PET_Heart_M0.nii.gz\n",
      "CB_102_PET_Heart_M1.nii.gz\n",
      "CB_105_PET_Heart_M1.nii.gz\n",
      "CB_108_PET_Heart_M1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Note to self! Only change ImgDepth in orientaitons other than axial. Otherwise interpolating unnecessarily!!\n",
    "PET_Images = LoadImages(ScanType=ScanType, ScanClass=\"Image\",\n",
    "                       ImgPath=IMGS_PATH, n_Scans=n_Scans, ImgSize=256, ImgDepth=256, Orientation=Orientation).LoadScans()\n",
    "PET_Masks = LoadImages(ScanType=ScanType, ScanClass=\"Mask\",\n",
    "                      MskPath=MSKS_PATH, n_Scans=n_Scans, ImgSize=256, ImgDepth=256, Orientation=Orientation).LoadScans()\n",
    "\n",
    "########################## Split Into Train and Test Set ##########################\n",
    "X, X_Val, y, y_Val = train_test_split(\n",
    "    PET_Images, PET_Masks, test_size=0.15, random_state=42)\n",
    "\n",
    "del PET_Images, PET_Masks\n",
    "\n",
    "y = tf.cast(y, dtype='float32')\n",
    "y_Val = tf.cast(y_Val, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Arrays with a 4'th Singular Dimension (Grayscale Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis=3)\n",
    "y = np.expand_dims(y, axis=3)\n",
    "X_Val = np.expand_dims(X_Val, axis=3)\n",
    "y_Val = np.expand_dims(y_Val, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Augmentation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAug = dict(rotation_range=15,\n",
    "               zoom_range=0.15,\n",
    "               horizontal_flip=True,\n",
    "               vertical_flip=True)\n",
    "\n",
    "image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**dataAug)\n",
    "mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**dataAug)\n",
    "seed = 42\n",
    "\n",
    "image_datagen.fit(X, augment=True, seed=seed)\n",
    "mask_datagen.fit(y, augment=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Augmented Scans Overlayed with Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_Aug = image_datagen.flow(X, batch_size=1, seed=seed)\n",
    "y_Aug = mask_datagen.flow(y, batch_size=1, seed=seed)\n",
    "viewImages = np.zeros((200, 256, 256, 1))\n",
    "viewMasks = np.zeros((200, 256, 256, 1))\n",
    "for i in range(199):\n",
    "    viewImages[i, :, :, :] = X_Aug.next()[0]\n",
    "    viewMasks[i, :, :, :] = y_Aug.next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "showCTMontageOverlay(IMG1=viewImages[0:199, :, :, 0], IMG2=viewMasks[0:199, :, :, 0],\n",
    "                     SIZE=25, SaveFig=False, save_fig_name=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Create U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Functions to Log Training of U-Net ##############\n",
    "def get_run_logdir(root_logdir, input_string):\n",
    "    import time\n",
    "    if not input_string:\n",
    "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    else:\n",
    "        run_id = os.path.join(\n",
    "            input_string, time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "\n",
    "def create_logdir(modelName):\n",
    "    root_logdir = os.path.join(os.curdir, \"My_logs\")\n",
    "    run_logdir = get_run_logdir(root_logdir, modelName)\n",
    "    return run_logdir\n",
    "################################################################\n",
    "\n",
    "########## Custom Loss Function for Dice Coeffiecient ##########\n",
    "# https://towardsdatascience.com/dealing-with-class-imbalanced-image-datasets-1cbd17de76b5\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
    "    y_true_pos = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_pos = tf.keras.backend.flatten(y_pred)\n",
    "    true_pos = tf.keras.backend.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = tf.keras.backend.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos = tf.keras.backend.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
    "\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=4/3):\n",
    "    tv = tversky(y_true, y_pred)\n",
    "    return tf.keras.backend.pow((1 - tv), gamma)\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Admin.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\My_logs\\U-Net_PET_Vanilla_Heart_Coronal\\run_2020_10_26-08_35_16\n",
      ".\\TrainedModels\\U-Net_PET_Vanilla_Heart_Coronal.h5\n",
      "U-Net_PET_Vanilla_Heart_Coronal\n"
     ]
    }
   ],
   "source": [
    "MyModelName = 'U-Net_PET_Vanilla_Heart_'+ Orientation\n",
    "MyLogdir = create_logdir(MyModelName)\n",
    "MyModelSaveRoot = os.path.join(os.curdir, \"TrainedModels\")\n",
    "MyModelSavePath = os.path.join(MyModelSaveRoot, MyModelName+\".h5\")\n",
    "\n",
    "print(MyLogdir)\n",
    "print(MyModelSavePath)\n",
    "print(MyModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del MyModel\n",
    "# importlib.reload(UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 64) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 64) 448         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 64) 448         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 128 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 128 896         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 128 147584      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 128 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 128 896         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 256)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 256)  1792        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 256)  1792        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 512)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 512)  3584        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 512)  2359808     batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 512)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 512)  3584        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 1024) 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 1024) 7168        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 1024) 9438208     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 1024) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 1024) 7168        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 16, 1024) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 1024) 0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 512)  4719104     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 512)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 512)  3584        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 1024) 0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 512)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 512)  3584        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 512)  2359808     batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 512)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 512)  3584        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 256)  1179904     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 256)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1792        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 512)  0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 256)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 256)  1792        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 256)  590080      batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 256)  0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 256)  1792        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 256 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 128 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 128 896         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 256 0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 128 295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 128 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 128 896         activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 128 147584      batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128, 128, 128 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128, 128, 128 896         activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 128 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 256, 256, 64) 448         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 128 0           batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 256, 256, 64) 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 256, 256, 64) 448         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 256, 256, 64) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 256, 256, 64) 448         activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 1)  65          batch_normalization_21[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 34,560,129\n",
      "Trainable params: 34,525,889\n",
      "Non-trainable params: 34,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MyModel = UNet.UNet_Vanilla(input_shape=(256, 256, 1)).CreateUnet()\n",
    "MyModel.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                loss=dice_coef_loss, metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger_cb = tf.keras.callbacks.CSVLogger(\n",
    "    os.path.join(MyModelSaveRoot, MyModelName+\".csv\"), append=True)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(MyModelSavePath,\n",
    "                                                   monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=15, restore_best_weights=True, monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(MyLogdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Proper Range for the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "steps_p_epoch = np.ceil(1000/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use small subset of data\n",
    "image_generator = image_datagen.flow(\n",
    "    X[0:1000, :, :, :], batch_size=batch_size, seed=seed)\n",
    "mask_generator = mask_datagen.flow(\n",
    "    y[0:1000, :, :, :], batch_size=batch_size, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(min_lr=1e-5, max_lr=1e-2,\n",
    "                     steps_per_epoch=steps_p_epoch, epochs=3)\n",
    "MyModel.fit(train_generator, steps_per_epoch=steps_p_epoch,\n",
    "            epochs=3, verbose=1, callbacks=[lr_finder])\n",
    "lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learningRates = \"0.001-0.006\"\n",
    "# 1st 0.0005-0.006\n",
    "# 2nd 0.00001-0.0005\n",
    "\n",
    "base_lr = 4e-4\n",
    "max_lr = 1e-3\n",
    "\n",
    "clr_triangular_cb = CyclicLR(\n",
    "    base_lr=base_lr, max_lr=max_lr, mode='triangular2', step_size=5*X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Compile Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer = \"SGD, Momentum = 0.9, Nestrov = True\"\n",
    "Optimizer = \"Adam\"\n",
    "loss = \"Dice_Loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Patameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_p_epoch = np.ceil(X.shape[0]/batch_size)\n",
    "image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 680.0 steps, validate on 960 samples\n",
      "Epoch 1/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5450 - mean_io_u: 0.6617\n",
      "Epoch 00001: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 566s 833ms/step - loss: 0.5452 - mean_io_u: 0.6616 - val_loss: 0.5936 - val_mean_io_u: 0.6468\n",
      "Epoch 2/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5191 - mean_io_u: 0.6662\n",
      "Epoch 00002: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 582s 856ms/step - loss: 0.5194 - mean_io_u: 0.6659 - val_loss: 0.5651 - val_mean_io_u: 0.6590\n",
      "Epoch 3/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5063 - mean_io_u: 0.6629\n",
      "Epoch 00003: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 612s 899ms/step - loss: 0.5070 - mean_io_u: 0.6628 - val_loss: 0.5469 - val_mean_io_u: 0.6253\n",
      "Epoch 4/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5303 - mean_io_u: 0.6651\n",
      "Epoch 00004: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 606s 892ms/step - loss: 0.5300 - mean_io_u: 0.6653 - val_loss: 0.5310 - val_mean_io_u: 0.6435\n",
      "Epoch 5/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5316 - mean_io_u: 0.6562\n",
      "Epoch 00005: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 571s 839ms/step - loss: 0.5318 - mean_io_u: 0.6561 - val_loss: 0.5493 - val_mean_io_u: 0.6548\n",
      "Epoch 6/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5094 - mean_io_u: 0.6640\n",
      "Epoch 00006: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 572s 841ms/step - loss: 0.5094 - mean_io_u: 0.6640 - val_loss: 0.5757 - val_mean_io_u: 0.6560\n",
      "Epoch 7/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5156 - mean_io_u: 0.6667\n",
      "Epoch 00007: val_loss did not improve from 0.47488\n",
      "680/680 [==============================] - 573s 843ms/step - loss: 0.5152 - mean_io_u: 0.6668 - val_loss: 0.5009 - val_mean_io_u: 0.6246\n",
      "Epoch 8/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4955 - mean_io_u: 0.6739\n",
      "Epoch 00008: val_loss improved from 0.47488 to 0.45614, saving model to .\\TrainedModels\\U-Net_PET_Vanilla_Heart_Coronal.h5\n",
      "680/680 [==============================] - 579s 851ms/step - loss: 0.4962 - mean_io_u: 0.6738 - val_loss: 0.4561 - val_mean_io_u: 0.6738\n",
      "Epoch 9/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4957 - mean_io_u: 0.6743\n",
      "Epoch 00009: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 568s 835ms/step - loss: 0.4965 - mean_io_u: 0.6741 - val_loss: 0.6605 - val_mean_io_u: 0.4999\n",
      "Epoch 10/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5137 - mean_io_u: 0.6516\n",
      "Epoch 00010: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 546s 803ms/step - loss: 0.5145 - mean_io_u: 0.6513 - val_loss: 0.6650 - val_mean_io_u: 0.4994\n",
      "Epoch 11/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4683 - mean_io_u: 0.6766\n",
      "Epoch 00011: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 546s 803ms/step - loss: 0.4691 - mean_io_u: 0.6765 - val_loss: 0.4956 - val_mean_io_u: 0.6687\n",
      "Epoch 12/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4686 - mean_io_u: 0.6815\n",
      "Epoch 00012: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 548s 805ms/step - loss: 0.4683 - mean_io_u: 0.6816 - val_loss: 0.4662 - val_mean_io_u: 0.6719\n",
      "Epoch 13/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4579 - mean_io_u: 0.6860\n",
      "Epoch 00013: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 551s 811ms/step - loss: 0.4573 - mean_io_u: 0.6860 - val_loss: 0.4838 - val_mean_io_u: 0.6700\n",
      "Epoch 14/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4995 - mean_io_u: 0.6751\n",
      "Epoch 00014: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 548s 807ms/step - loss: 0.4998 - mean_io_u: 0.6748 - val_loss: 0.4852 - val_mean_io_u: 0.6405\n",
      "Epoch 15/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5146 - mean_io_u: 0.6548\n",
      "Epoch 00015: val_loss did not improve from 0.45614\n",
      "680/680 [==============================] - 548s 807ms/step - loss: 0.5138 - mean_io_u: 0.6548 - val_loss: 0.6650 - val_mean_io_u: 0.4994\n",
      "Epoch 16/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.5033 - mean_io_u: 0.6503\n",
      "Epoch 00016: val_loss improved from 0.45614 to 0.44751, saving model to .\\TrainedModels\\U-Net_PET_Vanilla_Heart_Coronal.h5\n",
      "680/680 [==============================] - 575s 845ms/step - loss: 0.5027 - mean_io_u: 0.6506 - val_loss: 0.4475 - val_mean_io_u: 0.6782\n",
      "Epoch 17/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4452 - mean_io_u: 0.6917\n",
      "Epoch 00017: val_loss improved from 0.44751 to 0.44675, saving model to .\\TrainedModels\\U-Net_PET_Vanilla_Heart_Coronal.h5\n",
      "680/680 [==============================] - 573s 843ms/step - loss: 0.4449 - mean_io_u: 0.6918 - val_loss: 0.4467 - val_mean_io_u: 0.6839\n",
      "Epoch 18/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4370 - mean_io_u: 0.6940\n",
      "Epoch 00018: val_loss did not improve from 0.44675\n",
      "680/680 [==============================] - 567s 833ms/step - loss: 0.4367 - mean_io_u: 0.6941 - val_loss: 0.4579 - val_mean_io_u: 0.6635\n",
      "Epoch 19/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4460 - mean_io_u: 0.6869\n",
      "Epoch 00019: val_loss did not improve from 0.44675\n",
      "680/680 [==============================] - 557s 820ms/step - loss: 0.4458 - mean_io_u: 0.6870 - val_loss: 0.5008 - val_mean_io_u: 0.6619\n",
      "Epoch 20/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4676 - mean_io_u: 0.6865\n",
      "Epoch 00020: val_loss did not improve from 0.44675\n",
      "680/680 [==============================] - 586s 862ms/step - loss: 0.4670 - mean_io_u: 0.6866 - val_loss: 0.4663 - val_mean_io_u: 0.6640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2772b680708>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel.fit(train_generator, steps_per_epoch=steps_p_epoch, epochs=epochs, verbose=1, validation_data=(X_Val, y_Val),\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb, clr_triangular_cb, tensorboard_cb, csv_logger_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MyModel.load_weights(MyModelSavePath)\n",
    "clr_triangular_cb._reset()\n",
    "#base_lr = 1e-4\n",
    "#max_lr = 6e-4\n",
    "#clr_triangular_cb = CyclicLR(\n",
    "#    base_lr=base_lr, max_lr=max_lr, mode='triangular2', step_size=5*X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 680.0 steps, validate on 960 samples\n",
      "Epoch 1/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4287 - mean_io_u: 0.6887\n",
      "Epoch 00001: val_loss did not improve from 0.44675\n",
      "680/680 [==============================] - 594s 873ms/step - loss: 0.4280 - mean_io_u: 0.6887 - val_loss: 0.4900 - val_mean_io_u: 0.6336\n",
      "Epoch 2/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4642 - mean_io_u: 0.6732\n",
      "Epoch 00002: val_loss did not improve from 0.44675\n",
      "680/680 [==============================] - 627s 923ms/step - loss: 0.4650 - mean_io_u: 0.6732 - val_loss: 0.4561 - val_mean_io_u: 0.6363\n",
      "Epoch 3/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.3989 - mean_io_u: 0.7042\n",
      "Epoch 00003: val_loss improved from 0.44675 to 0.41901, saving model to .\\TrainedModels\\U-Net_PET_Vanilla_Heart_Coronal.h5\n",
      "680/680 [==============================] - 619s 910ms/step - loss: 0.3983 - mean_io_u: 0.7042 - val_loss: 0.4190 - val_mean_io_u: 0.6776\n",
      "Epoch 4/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4875 - mean_io_u: 0.6722\n",
      "Epoch 00004: val_loss did not improve from 0.41901\n",
      "680/680 [==============================] - 583s 857ms/step - loss: 0.4878 - mean_io_u: 0.6722 - val_loss: 0.5005 - val_mean_io_u: 0.6575\n",
      "Epoch 5/20\n",
      "679/680 [============================>.] - ETA: 0s - loss: 0.4614 - mean_io_u: 0.6888\n",
      "Epoch 00005: val_loss did not improve from 0.41901\n",
      "680/680 [==============================] - 599s 882ms/step - loss: 0.4616 - mean_io_u: 0.6887 - val_loss: 0.4450 - val_mean_io_u: 0.6787\n",
      "Epoch 6/20\n",
      "256/680 [==========>...................] - ETA: 8:57 - loss: 0.4568 - mean_io_u: 0.7014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_io_u\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-82cec62245fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m MyModel.fit(train_generator, steps_per_epoch=steps_p_epoch, epochs=epochs, verbose=1, validation_data=(X_Val, y_Val),\n\u001b[1;32m----> 2\u001b[1;33m             callbacks=[checkpoint_cb, early_stopping_cb, clr_triangular_cb, tensorboard_cb, csv_logger_cb])\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MyMasters\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "MyModel.fit(train_generator, steps_per_epoch=steps_p_epoch, epochs=epochs, verbose=1, validation_data=(X_Val, y_Val),\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb, clr_triangular_cb, tensorboard_cb, csv_logger_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_Val, y_Val),\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb, clr_triangular_cb, tensorboard_cb, csv_logger_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_triangular_cb._reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel.load_weights(MyModelSavePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Model Parameters to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModelParameters_Strings = [\"ScanType\", \"n_Scans\",\n",
    "                             \"Orientation\", \"Optimizer\", \"Loss\", \"batch_size\", \"epochs\"]\n",
    "MyModelParameters_values = [ScanType, n_Scans,\n",
    "                            Orientation, Optimizer, loss, batch_size, 25]\n",
    "\n",
    "TextFileName = MyModelName+\".txt\"\n",
    "TextFilePath = os.path.join(os.curdir, \"TrainedModels\", TextFileName)\n",
    "\n",
    "with open(TextFilePath, \"w\") as file:\n",
    "    file.write(\"Parameters for \" + MyModelName + \":\\n\\n\")\n",
    "    for parameter in enumerate(MyModelParameters_Strings):\n",
    "        file.write(parameter[1] + \": \" +\n",
    "                   str(MyModelParameters_values[parameter[0]])+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Performance on Test Set\n",
    "## View Predicted Images Over Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    y_predict = MyModel.predict(X_Val, batch_size=10, verbose=1)\n",
    "except:\n",
    "    X_Val = np.squeeze(X_Val)\n",
    "    print(\"Error: Input to Model has to be 4D (x, y, x, 1)\")\n",
    "    print(\"Reshaping..\")\n",
    "    X_Val = np.expand_dims(X_Val, axis=3)\n",
    "    y_predict = MyModel.predict(X_Val, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predict = np.squeeze(y_predict)\n",
    "X_Val = np.squeeze(X_Val)\n",
    "\n",
    "try:\n",
    "    X_Val = np.squeeze(X_Val)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    y_threshold = np.squeeze(y_threshold)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    y_Val = np.squeeze(y_Val)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "y_new = np.ma.masked_where(y_predict > 0, y_predict, copy=False)\n",
    "\n",
    "showCTMontageOverlay(IMG1=X_Val[0:250, :, :],\n",
    "                     IMG2=y_predict[0:250, :, :], SIZE=25, SaveFig=False, save_fig_name=\"Predicted Masks on Actual Masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Performance on Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LoadImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScanName = \"CB_130_PET_M0\"\n",
    "TestImage, Orig_Size, MetaData = LoadImages(ScanType=\"PET\", ScanClass=\"Image\", ScanName=ScanName+\".nii.gz\",\n",
    "                                            ImgPath=\"F:\\\\MyMasters\\\\Data\\\\TestingData\\\\PET\\\\imgs\", Orientation=Orientation).LoadScan()\n",
    "RunModels(OutPath=OUTPUT_PATH, ScanName=\"P\"+ScanName, Scan=TestImage, Scan_Size=Orig_Size,\n",
    "          Scan_Metadata=MetaData, Model=MyModel, Orientation=Orientation).runModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orig_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
